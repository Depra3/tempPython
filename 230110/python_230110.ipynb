{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 빅데이터 비즈니스 관점\n",
        "- 데이터 많은 회사가 결국 이김\n",
        "- 데이터를 활용\n",
        "  + OPEN API 형태로 돈을 제공\n",
        "- API 제공하는 최사는 굉장히 많음\n",
        "  + 빅테크 기업들은 거의 다 제공 중"
      ],
      "metadata": {
        "id": "H5enllZ1BfaU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcJqPtBwBbS-",
        "outputId": "c0253bf1-8358-46c4-c776-5f7ea3ff56fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Response [200]>\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "                                                                        # json으로 출력(xml로 변경시 xml로 출력함) \n",
        "url = \"http://data.ex.co.kr/openapi/trtm/realUnitTrtm?key=1308370912&type=json&iStartUnitCode=101&iEndUnitCode=103&numOfRows=100&pageNo=1\"\n",
        "req = requests.get(url)\n",
        "print(req)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### json 형태로 출력"
      ],
      "metadata": {
        "id": "G1mmYIENSmw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_result = req.json()\n",
        "json_result"
      ],
      "metadata": {
        "id": "XYHdPKhsSpJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터 가공"
      ],
      "metadata": {
        "id": "EI5M5SwUT3db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cars = json_result['realUnitTrtmVO']\n",
        "# len(cars)\n",
        "records = []\n",
        "for car in cars:\n",
        "  dic = {}\n",
        "  dic['date'] = car['stdDate']\n",
        "  dic['time'] = car['stdTime']\n",
        "  dic['timeAvg'] = car['timeAvg']\n",
        "  dic['cartype'] = car['tcsCarTypeDivName']\n",
        "  records.append(dic)\n",
        "\n",
        "records"
      ],
      "metadata": {
        "id": "vBfKhmsET4rZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(records)\n",
        "df\n",
        "# df.to_csv('data.csv')"
      ],
      "metadata": {
        "id": "Y9y6InPyU9uC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XML 활용"
      ],
      "metadata": {
        "id": "pHvj6yCwWBxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lxml\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "url = \"http://data.ex.co.kr/openapi/trtm/realUnitTrtm?key=1308370912&type=xml&iStartUnitCode=101&iEndUnitCode=103\"\n",
        "result = requests.get(url)\n",
        "content = result.content\n",
        "soup = BeautifulSoup(content, \"lxml\") # html.parser \n",
        "# print(soup)\n",
        "print(soup.prettify())\n",
        "\n",
        "timeAvg_lists = soup.find_all(\"timeavg\")\n",
        "stddate_lists = soup.find_all(\"stddate\")\n",
        "stdtime_lists = soup.find_all(\"stdtime\")\n",
        "\n",
        "avgTime = []\n",
        "stddate = []\n",
        "stdtime = []\n",
        "for timeAvg, date, time in zip(timeAvg_lists, stddate_lists, stdtime_lists):\n",
        "  print(timeAvg, date, time)\n",
        "  # print(time.get_text())\n",
        "  avgTime.append(timeAvg.get_text())\n",
        "  stddate.append(date.get_text())\n",
        "  stdtime.append(time.get_text())\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"date\": stddate, \n",
        "    \"time\": stdtime, \n",
        "    \"avgTime\": avgTime\n",
        "})\n"
      ],
      "metadata": {
        "id": "rW3HMKwqWPq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 네이버 주식 데이터 수집\n",
        "- 다중 페이지\n"
      ],
      "metadata": {
        "id": "qT5jAaw3chpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install fake_useragent\n",
        "# 가상 useragent 사용하기 위한 패키지. 사용해도 되고 안해도 되는 패키지.\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "# 035720 카카오\n",
        "# 005930 삼성\n",
        "# 004690 삼천리\n",
        "company_code = '005930'\n",
        "url = 'https://finance.naver.com/item/sise_day.naver?code=' + company_code\n",
        "# print(url)\n",
        "\n",
        "ua = UserAgent()\n",
        "\n",
        "custom_headers = {\n",
        "    'User-agent':ua.ie\n",
        "    # 'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
        "}\n",
        "\n",
        "response = requests.get(url, headers = custom_headers)\n",
        "response.status_code\n",
        "\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "# print(soup)\n",
        "\n",
        "last_page = int(soup.select_one('td.pgRR').a['href'].split('=')[2]) # -1은 맨 끝이니 -1로 대체 가능\n",
        "# print(last_page)\n",
        "\n",
        "df = None # 페이지별로 데이터프레임 누적 병합\n",
        "count = 0 # break문 작성용\n",
        "\n",
        "for page in range(1, last_page+1):\n",
        "  # urlp = url + '&page={}'.format(page)\n",
        "  # print(urlp)\n",
        "  res = requests.get(f'{url}&page={page}',headers = custom_headers)\n",
        "  # print(res)\n",
        "  df = pd.concat([df,pd.read_html(res.text, encoding='euc-kr')[0]], ignore_index=True)\n",
        "\n",
        "  if count > 2:\n",
        "    break\n",
        "  count += 1\n",
        "\n",
        "df.dropna(inplace=True) # drop na 를 이용해 결측지 제거\n",
        "df.reset_index(drop=True, inplace=True) \n",
        "# 행 인덱스를 재배열 함수 - drop=True : 기존의 인덱스를 버림 / inplace=True : 다른 변수에 입력하지 않고 df에 바로 적용\n",
        "df"
      ],
      "metadata": {
        "id": "YcWz5vj_ckWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 증시 기업 코드\n",
        "company_codes = [\"030200\", \"005930\", \"068270\", \"035720\"]\n",
        "company_names = [\"KT\", \"삼성전자\", \"셀트리온\", \"카카오\"]\n",
        "url = \"https://finance.naver.com/item/main.nhn?code=\" + company_code\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "nD = []\n",
        "\n",
        "for i in range(0, (len(company_codes))):\n",
        "  url = \"https://finance.naver.com/item/main.nhn?code=\"+company_codes[i]\n",
        "  custom_header = {\n",
        "      'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
        "  }\n",
        "\n",
        "  res = requests.get(url,headers = custom_header)\n",
        "  soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "  div = soup.find('div', class_ = 'today')\n",
        "  # print(div)\n",
        "\n",
        "  nV = div.find_all('span', class_ = 'blind')[0].get_text()\n",
        "  nD.append(nV)\n",
        "  # code = company_codes[i]\n",
        "  # name = company_names[i]\n",
        "  # nD = pd.DataFrame({\n",
        "      # '회사 코드' : [code],\n",
        "      # '회사 이름' : [name],\n",
        "      # '현재 가격' : nV\n",
        "  # })\n",
        "  # print(nD)\n",
        "  # df = pd.concat([df,nD], ignore_index=True)\n",
        "  # df = pd.concat([df,pd.read_html(res.text, encoding='euc-kr')[0]], ignore_index=True)\n",
        "\n",
        "dic = {\n",
        "    '회사 코드' : company_codes,\n",
        "    '회사 이름' : company_names,\n",
        "    '현재 가격' : nD\n",
        "}\n",
        "df = pd.DataFrame(dic)\n",
        "print(df)\n",
        "\n",
        "# df.reset_index(drop=True, inplace=True)\n",
        "# print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPYIoqFEqWNU",
        "outputId": "74d0da12-3994-49d5-8198-b9670cf0b7d6"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    회사 코드 회사 이름    현재 가격\n",
            "0  030200    KT   34,250\n",
            "1  005930  삼성전자   60,400\n",
            "2  068270  셀트리온  165,000\n",
            "3  035720   카카오   60,700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from fake_useragent import UserAgent\n",
        "from datetime import datetime\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "def get_Code(company_code):\n",
        "    url =\"https://finance.naver.com/item/main.nhn?code=\" + company_code\n",
        "    ua = UserAgent()\n",
        "    headers = { 'User-agent': ua.ie }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    return soup\n",
        " \n",
        "def get_Price(company_code):\n",
        "    soup = get_Code(company_code)\n",
        "    no_today = soup.select_one('p.no_today')\n",
        "    print(no_today)\n",
        "    blind = no_today.select_one('span.blind')\n",
        "    return blind.text\n",
        "\n",
        "def create_df(company_code, company_name, prices_list):\n",
        "  data = pd.DataFrame({\"종목코드\": company_code, \n",
        "                       \"상장회사\": company_name, \n",
        "                       \"주가\": prices_list})\n",
        "  return data\n",
        " \n",
        "# 증시 기업코드\n",
        "company_codes = [\"030200\", \"005930\", \"068270\", \"035720\"]\n",
        "company_names = [\"KT\", \"삼성전자\", \"셀트리온\", \"카카오\"]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    now = datetime.now()\n",
        "    print(\"-\" * 60)\n",
        "    print(now)\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    prices_list = []\n",
        "    for elm in company_codes:\n",
        "        nowPrice = get_Price(elm)\n",
        "        prices_list.append(nowPrice)\n",
        "\n",
        "    # print(get_prices)\n",
        "\n",
        "    stock_data = create_df(company_codes, company_names, prices_list)\n",
        "    print(stock_data)\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Selenium 방식 https://dschloe.github.io/python/crawling/python_selenium_crawling/"
      ],
      "metadata": {
        "id": "NFHPLKGf0Vvp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}